<!DOCTYPE HTML>
<html lang="en">
  <head>
    <!-- Basics -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Sreeharsha Paruchuri</title>
    <meta name="description" content="CMU MRSD graduate student focused on AI, Computer Vision, and Robotics. I build high-quality, scalable systems that bridge research and production.">

    <!-- Canonical & Robots -->
    <link rel="canonical" href="https://sreeharshaparuchur1.github.io/">
    <meta name="robots" content="index,follow,max-snippet:-1,max-image-preview:large,max-video-preview:-1">
    <meta name="google-site-verification" content="3iuIKut8-Qkv-yLKkyKC7iNF7hPGDG47Khx7w9kC1TA" />
    
    <!-- Branding -->
    <meta name="author" content="Sreeharsha Paruchuri">
    <meta name="theme-color" content="#0f172a">

    <!-- Open Graph (LinkedIn, Slack, etc.) -->
    <meta property="og:title" content="Sreeharsha Paruchuri">
    <meta property="og:description" content="CMU MRSD graduate student focused on AI, Computer Vision, and Robotics. I build high-quality, scalable systems that bridge research and production.">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://sreeharshaparuchur1.github.io/">
    <meta property="og:site_name" content="Sreeharsha Paruchuri">
    <meta property="og:locale" content="en_US">
    <meta property="og:image" content="https://sreeharshaparuchur1.github.io/images/profile_pictures/sreeharsha_profile_picture.JPG">
    <meta property="og:image:alt" content="Portrait of Sreeharsha Paruchuri">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    
    <!-- Favicons -->
    <link rel="shortcut icon" href="images/favicon/favi.png" type="image/x-icon">
    <link rel="apple-touch-icon" href="images/favicon/favi.png">
    
    <!-- Stylesheets -->
    <link rel="stylesheet" type="text/css" href="styles.css">
    
    <!-- Scripts -->
    <script src="script.js"></script>
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "Sreeharsha Paruchuri",
      "url": "https://sreeharshaparuchur1.github.io/",
      "image": "https://sreeharshaparuchur1.github.io/images/profile_pictures/sreeharsha_profile_picture.JPG",
      "jobTitle": "Graduate Student, Robotics (MRSD)",
      "affiliation": {
        "@type": "CollegeOrUniversity",
        "name": "Carnegie Mellon University"
      },
      "alumniOf": {
        "@type": "CollegeOrUniversity",
        "name": "Carnegie Mellon University"
      },
      "sameAs": [
        "https://www.linkedin.com/in/sreeharshaparuchur1/"
      ]
    }
    </script>
  </head>

  <body>
    <!-- Navigation Bar -->
    <div class="nav-container">
      <nav class="navbar">
        <a href="#about" class="nav-link">About</a>
        <a href="#education" class="nav-link">Education</a>
        <a href="#experience" class="nav-link">Experience</a>
        <a href="#projects" class="nav-link">Projects</a>
        <a href="#teaching" class="nav-link">Teaching</a>
        <a href="quotes.html" class="nav-link">Photography</a>
        <button class="theme-toggle" onclick="toggleTheme()">üåô</button>
      </nav>
    </div>

    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <div id="about" class="about-section">
            <div class="about-content">
              <div class="about-text">
                <p class="name" style="text-align: center;">
                  Sreeharsha Paruchuri
                </p>
                <p>
		Hi, I'm Harsha, a robotics graduate student at <a href="https://www.ri.cmu.edu/" target="_blank">Carnegie Mellon University's Robotics Institute</a>, pursuing the Master of Science in Robotic Systems Development (MRSD). I'm currently working on giving robots the ability to understand complex 3D environments and take informed decisions for long-horizon tasks. 
                </p>
    
    At IIIT-Hyderabad, I explored robotics and computer vision with <a href="https://www.iiit.ac.in/people/faculty/madhava-krishna" target="_blank">Prof. Madhava Krishna</a> and computational social science with <a href="https://www.iiit.ac.in/people/faculty/vinoo-alluri" target="_blank">Prof. Vinoo Alluri</a>, learning to approach problems from multiple angles and design interdisciplinary solutions. Currently, I'm deepening that foundation through MRSD's unique blend of coursework, systems engineering, and entrepreneurship training which challenge me to think beyond code and about scalability, reliability, and teamwork in building complex robotic platforms.
                </p>
                
                <!-- <p>
		At CMU, I'm deepening that foundation through MRSD's unique blend of AI-driven robotics coursework, systems engineering, and entrepreneurship training. My technical focus includes reinforcement learning for navigation, computer vision, and multimodal perception, while systems engineering and project management courses challenge me to think beyond code about scalability, reliability, and teamwork in building complex robotic platforms.
                </p> -->
                
                <p>
		Previously, I worked as a Pre-Doctoral Research Fellow at TCS Research where I worked on imbuing embodied AI agents with multimodal sensing capabilities through audio and visual inputs.
                </p>
                
                <p>
    My curiosity about how the world works trumps everything else - I love learning and thinking about new ways to solve pressing problems in society and humanity as a whole. I'm on the lookout for conversations and opportunities to hone and utilize my skills and would love to chat about ideas you may have :)
                 </p>

                <p>
		<strong>Research Interests:</strong> 3D Computer Vision and Long-Horizon Robot Intelligence for embodied agents.
                </p>
                <p style="text-align:center">
                  <a href="mailto:sparuchu@cs.cmu.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/sreeharshaparuchuri">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/sreeharshaparuchur1">Github</a> &nbsp;/&nbsp;
                  <a href="documents/sreeharsha_resume_cmu.pdf">Resume</a>
                </p>
              </div>
              <div class="profile-picture-container">
                <a href="images/profile_pictures/sreeharsha_profile_picture.JPG">
                  <img alt="profile photo" src="images/profile_pictures/sreeharsha_profile_picture.JPG">
                </a>
              </div>
            </div>
          </div>

          <!-- Education Section -->
          <table id="education" style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:12px;"><tbody>
            <tr>
              <td>
                <h2>Education</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
                          <tr class="section-row">
                <td class="section-image-cell">
                  <img src='images/logos/cmu.jpeg' alt="Carnegie Mellon University Logo" class="education-logo" onclick="window.open('https://www.ri.cmu.edu/', '_blank')">
                </td>
              <td class="section-content-cell">
                <div class="duration-container">
                  <div class="section-title-container">
                    <strong class="section-title">Carnegie Mellon University, School of Computer Science</strong>
                  </div>
                  <div class="duration-tag">Aug 2024 - May 2026</div>
                </div>
                <em>Master of Science in Robotic Systems Development (MRSD)</em><br>
                <strong>CGPA: 4.11/4.0</strong><br>
                <strong>Teaching:</strong> Introduction to Deep Learning (11-785)<br>
                <strong>Coursework:</strong> <span id="cmu-courses-short">Learning for 3D Vision, Generative AI, Deep RL</span><span id="cmu-courses-full">Learning for 3D Vision, Generative Artificial Intelligence, Deep Reinforcement Learning and Control, Advanced Computer Vision, Manipulation, Estimation and Control</span>
                <div class="experience-toggle">
                  <a href="#" onclick="toggleCourses('cmu'); return false;" id="cmu-toggle">Show more</a>
                </div>
              </td>
            </tr>

            <tr class="section-row">
                              <td class="section-image-cell">
                  <img src='images/logos/iiith-v3.png' alt="IIIT Hyderabad Logo" class="education-logo" onclick="window.open('https://www.iiit.ac.in/', '_blank')">
                </td>
              <td class="section-content-cell">
                <div class="duration-container">
                  <div class="section-title-container">
                    <strong class="section-title">International Institute of Information Technology, Hyderabad</strong>
                  </div>
                  <div class="duration-tag">Aug 2018 - July 2022</div>
                </div>
                <em>Bachelor of Technology in Electronics and Communication Engineering (Honours)</em><br>
                <strong>Major GCPA: 9.02/10</strong><br>
                <strong>Awards:</strong> Deans Merit List, Undergraduate Research Award<br>
                <strong>Coursework:</strong> <span id="iiith-courses-short">Statistics in AI, Topics in Applied Optimization, Mobile Robotics</span><span id="iiith-courses-full">Statistical Methods in Artificial Intelligence, Topics in Applied Optimization, Mobile Robotics, Data Structures and Algorithms, Digital Image Processing, Game Theory, Compilers, Operating Systems, Linear Algebra</span>
                <div class="experience-toggle">
                  <a href="#" onclick="toggleCourses('iiith'); return false;" id="iiith-toggle">Show more</a>
                </div>
              </td>
            </tr>
          </tbody></table>

                    <!-- Experience Section -->
          <table id="experience" style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:12px;"><tbody>
            <tr>
              <td>
                <h2>Industry Experience</h2>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr class="section-row">
              <td class="section-image-cell">
                <img src='images/logos/mach9-logo.jpeg' alt="Mach9 Logo" class="logo-image" onclick="window.open('https://www.mach9.ai/', '_blank')">
              </td>
              <td class="section-content-cell">
                <div class="duration-container">
                  <div class="section-title-container">
                    <strong class="section-title">Mach9</strong>
                  </div>
                  <div class="duration-tag">May 2025 - Aug 2025</div>
                </div>
                <em>Perception Software Engineering Intern</em><br>
                <strong>Focus:</strong> Developed CUDA-accelerated 2D-3D feature correspondence pipeline and fine-tuned Vision-Language Models to speed up quality assurance in outdoor surveying systems<br>
                <div class="experience-toggle">
                  <a href="#" onclick="toggleExperience('mach9-current'); return false;" id="mach9-current-toggle">View Details</a>
                </div>
                                  <div id="mach9-current-details" class="experience-details">
                    <div class="key-contributions">
                    <strong>Key Contributions:</strong><br>
                    ‚Ä¢ Developed and deployed Pavement Symbol Extraction functionality to the Digital Surveyor software via <strong>CUDA-accelarated coordinate frame transformations</strong> and segmentation masks.<br>
                    ‚Ä¢ Fine-tuned a <strong>Vision-Language Model (VLM)</strong> to implement a secondary-inference pipeline to classify extracted open-set painted symbols according to user specifications.<br>
                    ‚Ä¢ Conducted 70+ controlled ablation experiments with <strong>A/B testing on Hungarian Assigner</strong> Costs, Loss weights, Model Queries, Multi-Scale Deformable Attention and encoder-decoder expressivity to boost the performance of the production model by 4%.<br>
                    ‚Ä¢ Utilised methods from Object-Detection literature to qualitatively capture a <strong>DETR-based</strong> polyline detection model's <strong>uncertainty</strong> to expedite downstream <strong>Quality Assurance</strong> and Quality Control processes, saving company and customer resources.<br>
                    <!-- <strong>Multimodal Symbol Extraction:</strong> Designed self-correcting vector-field and DBSCAN clustering algorithm to preserve instance consistency from multi-view panoptic masks. Accelerated per-point transformations by 50√ó via custom CUDA kernel<br>
                    ‚Ä¢ <strong>Vision-Language Model Inference:</strong> Designed orientation robust RAG pipeline for symbol classification using Gemini text embeddings and GPT-o3, achieving 85% F1 score on 20k samples<br>
                    ‚Ä¢ <strong>Uncertainty Estimation:</strong> Developed methods to quantify uncertainty in DETR-style vectorized 3D polyline predictions via self-calibration and Bayesian dropout -->
                    </div>
                  </div>
              </td>
            </tr>

            <tr class="section-row">
              <td class="section-image-cell">
                <img src='images/logos/tcs-logo.jpeg' alt="TCS Research Logo" class="logo-image" onclick="window.open('https://www.tcs.com/what-we-do/research', '_blank')">
              </td>
              <td class="section-content-cell">
                <div class="duration-container">
                  <div class="section-title-container">
                    <strong class="section-title">Tata Consultancy Services Research</strong>
                  </div>
                  <div class="duration-tag">July 2022 - July 2024</div>
                </div>
                <em>Pre-Doctoral Research Fellow</em><br>
                <strong>Focus:</strong> Led research in embodied AI navigation focusing on audio-visual feature correspondence and reinforcement learning<br>
                <div class="experience-toggle">
                  <a href="#" onclick="toggleExperience('tcs'); return false;" id="tcs-toggle">View Details</a>
                </div>
                <div id="tcs-details" class="experience-details">
                  <div class="key-contributions">
                  <strong>Key Contributions:</strong><br>
                  ‚Ä¢ <strong>Audio-Visual Navigation:</strong> Led development of embodied AI agent with multimodal sensing, training online RL policy with novel class-agnostic reward, reducing path length by 21%<br>
                  ‚Ä¢ <strong>Offline RL for Indoor Robot Navigation:</strong> Built simulation pipeline to collect large-scale trajectory datasets, training a Causal Decision Transformer with early multimodal fusion; integrated environment randomization, behavior cloning baselines, and replay buffer curation to improve policy robustness and sample efficiency
                  ‚Ä¢ <strong>CLIP-Enhanced Scene Graphs:</strong> Designed contrastive-learning framework to compute visual-language embeddings, leveraging GNNs to model object-region relationships<br>
                  ‚Ä¢ <strong>Open Vocabulary Manipulation (NeurIPS 23):</strong> Developed active SLAM exploration algorithm conditioned on probabilistic semantic map, improving task success by 60%
                  ‚Ä¢ Volunteered for the Project Synergy initiative by TCS wherein volunteers taught written and spoken English to students in a Bangla-medium government school.
                  </div>
                </div>
              </td>
            </tr>

            <tr class="section-row">
              <td class="section-image-cell">
                <img src='images/logos/rrc_logo.png' alt="RRC IIIT-H Logo" class="logo-image" onclick="window.open('https://robotics.iiit.ac.in/', '_blank')">
              </td>
              <td class="section-content-cell">
                <div class="duration-container">
                  <div class="section-title-container">
                    <strong class="section-title">Robotics Research Center (RRC, IIIT-H)</strong>
                  </div>
                  <div class="duration-tag">Jan 2020 - June 2022</div>
                </div>
                <em>Research Assistant</em><br>
                <strong>Focus:</strong> Worked on dense 3D reconstruction and VSLAM related projects for indoor and outdoor autonomy<br>
                <div class="experience-toggle">
                  <a href="#" onclick="toggleExperience('rrc'); return false;" id="rrc-toggle">View Details</a>
                </div>
                <div id="rrc-details" class="experience-details">
                  <div class="key-contributions">
                  <strong>Key Contributions:</strong><br>
                  ‚Ä¢ <strong>Autonomous Sanitization Robot:</strong> Designed and implemented end-to-end robotic system during COVID-19 to autonomously sanitize indoor spaces, integrating computer vision, Visual-SLAM, and coverage-based navigation<br>
                  ‚Ä¢ <strong>Sim-to-Real Deployment:</strong> Built Gazebo simulation environments for iterative testing, then transferred stack to hardware platform with onboard sensors and sanitization actuators; finished <em>runner-up among 140 teams</em><br>
                  ‚Ä¢ <strong>LiDAR SLAM:</strong> Evaluated LiDAR odometry and mapping approaches such as LOAM using CARLA simulation and outdoor driving data, analyzing localization accuracy and map consistency<br>
                  ‚Ä¢ <strong>Depth Estimation:</strong> Implemented stereo and monocular depth estimation methods on driving datasets including KITTI and NuScenes, developing a ROS package for multi-view bundle adjustment<br>                  
                  </div>
                </div>
              </td>
            </tr>
          </tbody></table>

          <!-- Additional Experience Subsection -->
          <div class="additional-experience-section">
            <h3 style="text-align: center; font-size: 18px; font-weight: 400; color: var(--text-color); margin: 20px 0 15px 0;">I have also been a part of</h3>
            <div class="additional-logos-container">
              <div class="additional-item">
                <img src="images/logos/bosch_logo.png" alt="Bosch Research" class="additional-logo" onclick="toggleAdditional('bosch')" id="bosch-logo">
                <div id="bosch-details" class="additional-details">
                  <div class="duration-container">
                    <div class="section-title-container">
                      <strong class="section-title">Bosch Research and Technology Center</strong>
                    </div>
                    <div class="duration-tag">May 2021 - Aug 2021</div>
                  </div>
                  <em>Computer Vision Intern</em><br>
                  <!-- <strong>Focus:</strong> Developed sensor fusion algorithms for autonomous driving, improving Multi-Object Tracking with Kalman filtering and synthetic data augmentation<br> -->
                  <div class="key-contributions">
                  <strong>Key Contributions:</strong><br>
                  ‚Ä¢ Fused Laser, Camera, and Odometry data using Kalman filtering to boost online Multi-Object Tracking performance by 11% IoU on outdoor autonomous driving datasets<br>
                  ‚Ä¢ Augmented difficult-to-obtain real-world LiDAR datasets using synthetic data from generative models and physics engines, improving 3D object detection networks for outdoor scenarios
                  </div>
                </div>
              </div>
              
              <div class="additional-item">
                <img src="images/logos/precog_logo-800.webp" alt="PReCoG Lab" class="additional-logo" onclick="toggleAdditional('precog')" id="precog-logo">
                <div id="precog-details" class="additional-details">
                  <div class="duration-container">
                    <div class="section-title-container">
                      <strong class="section-title">PreCog Lab, IIIT-H</strong>
                    </div>
                    <div class="duration-tag">2020 - 2021</div>
                  </div>
                  <em>Research Assistant - Information Retrieval and Computational Social Science</em><br>
                  <!-- <strong>Focus:</strong> Applied machine learning to music information retrieval and social media analysis for mental health research, resulting in publications at top-tier venues<br> -->
                  <div class="key-contributions">
                  <strong>Key Contributions:</strong><br>
                  ‚Ä¢ Applied statistical machine learning with Music Information Retrieval to analyze lyrical regularities as early indicators of mental illness; Published results at INTERSPEECH 2021<br>
                  ‚Ä¢ Scraped Reddit data to link music-sharing trends with mental health during COVID-19 using BERT embeddings and DBSCAN clustering; Published in medical journal
                  </div>
                </div>
              </div>
            </div>
          </div>

          <!-- Projects/Research Section -->
          <table id="projects" style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:12px;"><tbody>
            <tr>
              <td>
                <h2>Projects & Research</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr style="background-color: var(--highlight-bg);">
      <td style="padding:16px;width:40%;vertical-align:middle">
        <img src="images/projects/capstone_1.png" alt="AR Knee Surgery Project" class="project-image">
      </td>
              <td style="padding:8px;width:60%;vertical-align:middle">
          <div class="duration-container">
            <div class="section-title-container">
              <strong class="section-title">Augmented-Reality and Robot Assisted Knee Surgery</strong>
            </div>
            <!-- <div class="duration-tag">MRSD Capstone</div> -->
          </div>
          <a href="https://mrsdprojects.ri.cmu.edu/2025teamd/">website</a> / <a href="https://github.com/KNEEpoleon">code</a><br>
          Gathered and analyzed requirements from user studies, market competition, and sponsors to inform system development. Processed 3D and RGB information from the Apple Vision Pro to detect bone models in the environment via ICP registration.
          <div class="experience-toggle" style="margin-top: 8px;">
            <a href="#" onclick="toggleExperience('ar-knee'); return false;" id="ar-knee-toggle">View Details</a>
          </div>
          <div id="ar-knee-details" class="experience-details">
            <div class="key-contributions">
              <strong>Key Contributions:</strong><br>
              ‚Ä¢ <strong>Project Leadership:</strong> Led a 5-person team as Project Manager, driving scheduling, sponsor communication, and system integration for an AR-assisted surgical robotics platform<br>
              ‚Ä¢ <strong>Accuracy-driven Perception:</strong> Achieved sub-4 mm drilling accuracy in total knee arthroplasty using a KUKA MED7 arm with multi-stage pointcloud registration (SAM2 + ICP)<br>
              ‚Ä¢ <strong>AR Integration:</strong> Integrated Apple Vision Pro for dynamic bone tracking and real-time surgeon-in-the-loop planning across long surgical horizons<br>
              ‚Ä¢ <strong>Motion Planning:</strong> Designed and deployed a ROS + MoveIt planning subsystem that adaptively updates as surgical pins are drilled, enabling safe trajectory generation<br>
              ‚Ä¢ <strong>Hardware Development:</strong> Built a custom 3D-printed drill end-effector with embedded control electronics, activated via trajectory execution for autonomous drilling
            </div>
          </div>
        </td>
    </tr>

    <tr>
      <td style="padding:16px;width:40%;vertical-align:middle">
        <img src='images/projects/ReMOV3R.png' alt="3D Foundation Models Project" class="project-image">
      </td>
      <td style="padding:8px;width:60%;vertical-align:middle">
        <div class="duration-container">
          <div class="section-title-container">
            <strong class="section-title">3D Foundation-Models for Monocular Video Reconstruction</strong>
          </div>
          <!-- <div class="duration-tag">CMU</div> -->
        </div>
        <a href="https://drive.google.com/file/d/1GlR9XqQvIqDgGwtfmFA7U9m0eOH5hUrs/view?usp=sharing">report</a> / <a href="https://github.com/sreeharshaparuchur1/Mov3r-L3D-project">code</a><br>
        Implemented semantic-geometric feature fusion using cross-attention between foundation model embeddings (DINOv2, Depth Anything) in a hierarchical state representation to recover camera extrinsics. Devised an adaptive keyframe selection strategy for confidence-aware pointmap refinement using a DUST3R-style architecture.
        <div class="experience-toggle" style="margin-top: 8px;">
          <a href="#" onclick="toggleExperience('3d-foundation'); return false;" id="3d-foundation-toggle">View Details</a>
        </div>
        <div id="3d-foundation-details" class="experience-details">
          <div class="key-contributions">
            <strong>Key Contributions:</strong><br>
            ‚Ä¢ <strong>Foundation Model Fusion:</strong> Designed cross-attention mechanism to combine DINOv2 semantic features with Depth Anything geometric priors, achieving robust 3D scene understanding from monocular video<br>
            ‚Ä¢ <strong>Adaptive Keyframe Selection:</strong> Developed confidence-aware algorithm that dynamically selects optimal frames for reconstruction, improving pointmap quality by 30% over uniform sampling<br>
            ‚Ä¢ <strong>DUST3R Architecture:</strong> Implemented hierarchical state representation with multi-scale feature pyramids to handle camera motion estimation and dense 3D reconstruction simultaneously
          </div>
        </div>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:40%;vertical-align:middle">
        <img src='images/projects/cmu_vln_1.png' alt="CMU VLA Challenge" class="project-image">
      </td>
      <td style="padding:8px;width:60%;vertical-align:middle">
        <div class="duration-container">
          <div class="section-title-container">
            <strong class="section-title">CMU VLA Challenge</strong>
          </div>
        </div>
        <a href="https://www.ai-meets-autonomy.com/cmu-vla-challenge">problem</a> / <a href="https://github.com/gupta-ish/CMU-VLA-Challenge/tree/team-copypasta-submission">code</a><br>
        Built a Vision-Language Navigation (VLN) system that answered natural language queries by combining <code>Gemini 2.5 Pro</code> embodied reasoning with a custom ROS state machine. The system produced numerical answers, object references, or waypoint plans under a strict 10-minute limit.
        <div class="experience-toggle" style="margin-top: 8px;">
          <a href="#" onclick="toggleExperience('vla-challenge'); return false;" id="vla-challenge-toggle">View Details</a>
        </div>
        <div id="vla-challenge-details" class="experience-details">
          <div class="key-contributions">
            <strong>Key Challenges & Solutions:</strong><br>
            ‚Ä¢ **Ambiguous spatial queries** (‚Äúclosest to the window‚Äù): used <code>Gemini 2.5 Pro</code> to classify and reason over spatial relations<br>
            ‚Ä¢ **Strict 10-minute runtime**: designed a ROS state machine to coordinate exploration, mapping, and answering with timed transitions<br>
            ‚Ä¢ **Constraint-aware navigation**: developed waypoint planners that leveraged semantic object information to satisfy ‚Äúbetween/avoid‚Äù instructions
          </div>
        </div>
      </td>
    </tr>
    

<!-- 
    <tr>
      <td style="padding:16px;width:40%;vertical-align:middle">
        <img src='images/projects/cmu_vln_1.png' alt="CMU VLA Challenge" class="project-image">
      </td>
      <td style="padding:8px;width:60%;vertical-align:middle">
        <div class="duration-container">
          <div class="section-title-container">
            <strong class="section-title">CMU VLA Challenge ‚Äì Vision-Language Navigation</strong>
          </div>
        </div>
        <a href="#">report</a> / <a href="https://github.com/gupta-ish/CMU-VLA-Challenge/tree/team-copypasta-submission">code</a><br>
        Built a <code>Vision-Language Navigation (VLN)</code> system for CMU‚Äôs embodied AI competition. The project combined <code>Gemini 2.5 Pro</code> for semantic reasoning with a custom ROS state machine that handled exploration, scene understanding, and navigation under strict timing constraints. Our system translated natural language queries into actionable plans, producing numerical answers, object references, or waypoint trajectories.
        <div class="experience-toggle" style="margin-top: 8px;">
          <a href="#" onclick="toggleExperience('vla-challenge'); return false;" id="vla-challenge-toggle">View Details</a>
        </div>
        <div id="vla-challenge-details" class="experience-details">
          <div class="key-contributions">
            <strong>Key Contributions:</strong><br>
            ‚Ä¢ Faced the challenge of interpreting free-form language like ‚Äúgo to the chair closest to the window‚Äù; solved it by integrating <code>Gemini 2.5 Pro</code> for classification and spatial relation reasoning, then routing outputs to specialized solvers<br>
            ‚Ä¢ Needed to operate under a <strong>10-minute exploration + answering window</strong>; addressed this by designing a ROS state machine that sequenced initialization, active exploration, and answering phases with strict timeout management<br>
            ‚Ä¢ Required navigation that respected semantic constraints (‚Äúbetween tables‚Äù, ‚Äúavoid the window‚Äù); implemented coverage exploration and waypoint planners that intelligently used available object semantics to generate feasible paths<br>
          </div>
        </div>
      </td>
    </tr> -->
    


    <tr>
      <td style="padding:16px;width:40%;vertical-align:middle">
        <img src='images/projects/poseGraph.png' alt="Pose Graph Optimization Project" class="project-image">
      </td>
      <td style="padding:8px;width:60%;vertical-align:middle">
        <div class="duration-container">
          <div class="section-title-container">
            <strong class="section-title">Pose Graph Optimization for 2D SLAM</strong>
          </div>
        </div>
        <a href="link-to-report">report</a> / <a href="link-to-code">code</a><br>
        Implemented a 2D SLAM backend where noisy odometry and loop closure constraints were refined into a globally consistent trajectory. Used <code>jax</code> to compute residuals and Jacobians, applied nonlinear least-squares optimization, and validated improvements with RPY and APE error metrics. Explored the role of confidence weighting in the information matrix and compared against <code>g2o</code> optimization with robust kernels.
        <div class="experience-toggle" style="margin-top: 8px;">
          <a href="#" onclick="toggleExperience('pose-graph'); return false;" id="pose-graph-toggle">View Details</a>
        </div>
        <div id="pose-graph-details" class="experience-details">
          <div class="key-contributions">
            <strong>Key Contributions:</strong><br>
            ‚Ä¢ <strong>Iterative Optimization:</strong> Built custom nonlinear solver in JAX with residual and Jacobian computation for pose updates<br>
            ‚Ä¢ <strong>Information Matrix Analysis:</strong> Studied effect of varying odometry vs loop closure confidence weights on trajectory quality<br>
            ‚Ä¢ <strong>Error Evaluation:</strong> Quantified improvements via RPY drift and Absolute Pose Error reduction compared to initial odometry<br>
            ‚Ä¢ <strong>g2o Benchmarking:</strong> Ran Cauchy, PseudoHuber, and Huber kernels in <code>g2o_viewer</code> to compare optimization strategies<br>
            ‚Ä¢ <strong>Literature Review:</strong> Analyzed ‚ÄúPast, Present & Future of SLAM‚Äù survey, contextualizing open problems in robustness and scalability with deep learning-based approaches
          </div>
        </div>
      </td>
    </tr>
    

    <tr style="background-color: var(--highlight-bg);">
      <td style="padding:16px;width:40%;vertical-align:middle">
        <img src='images/projects/reddit_2.png' class="project-image">
      </td>
      <td style="padding:8px;width:60%;vertical-align:middle">
        <div class="duration-container">
          <div class="section-title-container">
            <strong class="section-title">Music, Mental Health, and Representation Learning</strong>
          </div>
          <!-- <div class="duration-tag">IIIT-H</div> -->
        </div>
        <a href="https://pubmed.ncbi.nlm.nih.gov/37471415/">publication</a> / <a href="#">code</a><br>
        Applied BERT-based sentiment analysis and k-means clustering to uncover nuanced links between language and acoustic music features in data scraped from mental health related subreddits during COVID-19. This research contributed to understanding the relationship between music and mental health through computational methods.
        <div class="experience-toggle" style="margin-top: 8px;">
          <a href="#" onclick="toggleExperience('music-mental-health'); return false;" id="music-mental-health-toggle">View Details</a>
        </div>
        <div id="music-mental-health-details" class="experience-details">
          <div class="key-contributions">
            <strong>Key Contributions:</strong><br>
            ‚Ä¢ <strong>BERT Sentiment Analysis:</strong> Fine-tuned transformer models on mental health discourse to extract emotional patterns from 50k+ Reddit posts, achieving 87% accuracy in mood classification<br>
            ‚Ä¢ <strong>Music Information Retrieval:</strong> Developed acoustic feature extraction pipeline using librosa and essentia to correlate musical elements (tempo, key, valence) with psychological states<br>
            ‚Ä¢ <strong>COVID-19 Impact Study:</strong> Applied k-means clustering and statistical analysis to identify significant behavioral shifts in music consumption patterns during pandemic, published findings at INTERSPEECH 2021
          </div>
        </div>
      </td>
    </tr>

          </tbody></table>

          <!-- View More Projects Link -->
          <div class="view-more-container">
            <a href="projects.html" class="view-more-link">View More Projects ‚Üí</a>
          </div>

          <!-- Teaching Section -->
          <table id="teaching" style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:12px;"><tbody>
            <tr>
              <td>
                <h2>Teaching Experience</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr class="section-row">
              <td class="section-image-cell">
                <img src='images/logos/cmu.jpeg' alt="Carnegie Mellon University Logo" class="education-logo" onclick="window.open('https://deeplearning.cs.cmu.edu/', '_blank')">
              </td>
              <td class="section-content-cell">
                <div class="duration-container">
                  <div class="section-title-container">
                    <strong class="section-title">Carnegie Mellon University</strong>
                  </div>
                  <!-- <div class="duration-tag">Fall 2025</div> -->
                </div>
                <em>Introduction to Deep Learning (11-785)</em><br>
                <a href="https://deeplearning.cs.cmu.edu/" target="_blank">Course Website</a><br>
                <strong>Description:</strong> Comprehensive graduate-level course covering neural networks, CNNs, RNNs, transformers, and modern deep learning architectures.
                <div class="experience-toggle">
                  <a href="#" onclick="toggleExperience('cmu-teaching'); return false;" id="cmu-teaching-toggle">View Details</a>
                </div>
                <div id="cmu-teaching-details" class="experience-details">
                  <div class="key-contributions">
                    <strong>Key Contributions:</strong><br>
                    ‚Ä¢ Created educational content including slides and tutorials for <a href="https://www.youtube.com/watch?v=pAVnXfgRJIA" target="_blank">NumPy fundamentals</a> and <a href="https://www.youtube.com/watch?v=yTQSvcuRHro" target="_blank">Loss functions (Focal Loss, Chamfer Loss, RLHF)</a><br>
                    ‚Ä¢ Collaborated with instructional team to revise and update homework assignments for RNNs, GRUs, Transformers, Language Generation, and Diffusion models<br>
                    ‚Ä¢ Conducted over 40 hours of office hours, labs, and hackathon events, providing hands-on instruction and problem-solving support for undergraduate and graduate students
                  </div>
                </div>
              </td>
            </tr>

            <tr class="section-row">
              <td class="section-image-cell">
                <img src='images/logos/iiith-v3.png' alt="IIIT Hyderabad Logo" class="education-logo" onclick="window.open('https://www.iiit.ac.in/', '_blank')">
              </td>
              <td class="section-content-cell">
                <div class="duration-container">
                  <div class="section-title-container">
                    <strong class="section-title">International Institute of Information Technology, Hyderabad</strong>
                  </div>
                  <!-- <div class="duration-tag">2020 - 2022</div> -->
                </div>
                <em>Multiple Courses</em><br>
                <strong>Courses:</strong> Mobile Robotics, Music Mind and Technology, Introduction to Coding Theory, RRC Summer School<br>
                <div class="experience-toggle">
                  <a href="#" onclick="toggleExperience('iiith-teaching'); return false;" id="iiith-teaching-toggle">View Details</a>
                </div>
                <div id="iiith-teaching-details" class="experience-details">
                  <div class="key-contributions">
                    <strong>Course Details:</strong><br>
                    ‚Ä¢ <strong>CS7.503.M21: Mobile Robotics</strong>: The most renowned course of IIIT-H across international universities. Provides students with a comprehensive toolkit for research at the intersection of Robotics and Computer Vision, covering SLAM algorithms and classical Computer Vision techniques<br>
                    ‚Ä¢ <strong>CS9.434.S22: Music, Mind and Technology</strong>: An interdisciplinary course using algorithms and mathematics to explore how music is perceived by individuals and groups. Served as head TA, designing evaluations for over 60 graduate and undergraduate students<br>
                    ‚Ä¢ <strong>EC5.205.S21: Introduction to Coding Theory</strong>: A fascinating subject building on Shannon's Theory of Communication, exploring the mathematical foundations that underpin everyday communication systems<br>
                    ‚Ä¢ <strong>RRC Summer School</strong>: Taught students the fundamentals of classical Computer Vision and introduced modern AI-based methods utilizing convolutional neural networks during this intensive summer program
                  </div>
                </div>
              </td>
            </tr>
          </tbody></table>

          <!-- News Section -->
          <table id="news" style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:12px;"><tbody>
            <tr>
              <td>
                <h2>News</h2>
              </td>
            </tr>
          </tbody></table>
          <div class="news-container">
            <div class="news-item">
              <span class="news-date">Aug 2025:</span> Completed my Summer Internship at Mach9 in the Bay Area üöÄ
            </div>
            <div class="news-item">
              <span class="news-date">Aug 2024:</span> Started Graduate School at CMU in the Robotics Institute! ü§ñ
            </div>
            <div class="news-item">
              <span class="news-date">June 2024:</span> Submitted our paper on Audio-Visual Navigation to CoRL 2024 ü§û
            </div>
            <div class="news-item">
              <span class="news-date">Feb 2024:</span> Admitted to CMU's MRSD program!
            </div>
            <div class="news-item">
              <span class="news-date">Feb 2024:</span> Invited to Google Research Week 2024 in Bangalore, India üéì
            </div>
            <div class="news-item">
              <span class="news-date">Nov 2023:</span> Finished 4th place internationally in the Habitat <a href="https://aihabitat.org/challenge/2023_homerobot_ovmm/">Open Vocabulary Mobile Manipulation Challenge</a> at NeurIPS 2023
            </div>
            <div class="news-item">
              <span class="news-date">July 2022:</span> Started working at TCS Research, Kolkata on Long-Horizon Robot Navigation using Deep Reinforcement Learning.
            </div>
            <div class="news-item">
              <span class="news-date">July 2022:</span> Graduated from IIIT-H with honours in Robot Perception! Thank you to my family and friends who made it possible ‚ú®
            </div>
          </div>

          <!-- Skills Section -->
          <table id="skills" style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Technical Skills</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px;">
                  <div>
                    <strong>Languages:</strong> Python, C++, MATLAB, CUDA, Java, Go, Swift<br>
                    <strong>ML/AI:</strong> PyTorch, TensorFlow, Scikit-learn, PyTorch3D
                  </div>
                  <div>
                    <strong>Tools:</strong> ROS2, Unity 3D, OpenCV, XCode, Django<br>
                    <strong>Miscellaneous:</strong> Rust, JAX
                  </div>
                </div>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Original Template taken from <a href="https://github.com/jonbarron/jonbarron_website">here!</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html> 