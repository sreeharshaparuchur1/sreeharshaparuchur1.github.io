<!DOCTYPE HTML>
<html lang="en">
  <head>
    <!-- Basics -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Sreeharsha Paruchuri</title>
    <meta name="description" content="CMU MRSD graduate student focused on AI, Computer Vision, and Robotics. I build high-quality, scalable systems that bridge research and production.">

    <!-- Canonical & Robots -->
    <link rel="canonical" href="https://sreeharshaparuchur1.github.io/">
    <meta name="robots" content="index,follow,max-snippet:-1,max-image-preview:large,max-video-preview:-1">
    <meta name="google-site-verification" content="3iuIKut8-Qkv-yLKkyKC7iNF7hPGDG47Khx7w9kC1TA" />
    
    <!-- Branding -->
    <meta name="author" content="Sreeharsha Paruchuri">
    <meta name="theme-color" content="#0f172a">

    <!-- Open Graph (LinkedIn, Slack, etc.) -->
    <meta property="og:title" content="Sreeharsha Paruchuri">
    <meta property="og:description" content="CMU MRSD graduate student focused on AI, Computer Vision, and Robotics. I build high-quality, scalable systems that bridge research and production.">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://sreeharshaparuchur1.github.io/">
    <meta property="og:site_name" content="Sreeharsha Paruchuri">
    <meta property="og:locale" content="en_US">
    <meta property="og:image" content="https://sreeharshaparuchur1.github.io/images/profile_pictures/sreeharsha_profile_picture.JPG">
    <meta property="og:image:alt" content="Portrait of Sreeharsha Paruchuri">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    
    <!-- Favicons -->
    <link rel="shortcut icon" href="images/favicon/favi.png" type="image/x-icon">
    <link rel="apple-touch-icon" href="images/favicon/favi.png">
    
    <!-- Stylesheets -->
    <link rel="stylesheet" type="text/css" href="styles.css">
    
    <!-- Scripts -->
    <script src="script.js"></script>
    
    <!-- Structured Data -->
    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "Sreeharsha Paruchuri",
      "url": "https://sreeharshaparuchur1.github.io/",
      "image": "https://sreeharshaparuchur1.github.io/images/profile_pictures/sreeharsha_profile_picture.JPG",
      "jobTitle": "Graduate Student, Robotics (MRSD)",
      "affiliation": {
        "@type": "CollegeOrUniversity",
        "name": "Carnegie Mellon University"
      },
      "alumniOf": {
        "@type": "CollegeOrUniversity",
        "name": "Carnegie Mellon University"
      },
      "sameAs": [
        "https://www.linkedin.com/in/sreeharshaparuchur1/"
      ]
    }
    </script>
  </head>

  <body>
    <!-- Navigation Bar -->
    <div class="nav-container">
      <nav class="navbar">
        <a href="#about" class="nav-link">About</a>
        <a href="#experience" class="nav-link">Experience</a>
        <a href="#projects" class="nav-link">Projects</a>
        <a href="#education" class="nav-link">Education</a>
        <a href="#teaching" class="nav-link">Teaching</a>
        <a href="quotes.html" class="nav-link">Photography</a>
        <button class="theme-toggle" onclick="toggleTheme()">üåô</button>
      </nav>
    </div>

    <table style="width:100%;max-width:1200px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <div id="about" class="about-section">
            <div class="about-content">
              <div class="about-text">
                <p class="name" style="text-align: center;">
                  Sreeharsha Paruchuri
                </p>
                <p>
		Hi, I'm Harsha, a robotics graduate student at <a href="https://www.ri.cmu.edu/" target="_blank">Carnegie Mellon University's Robotics Institute</a>, pursuing the Master of Science in Robotic Systems Development (MRSD). I'm currently working on giving robots the ability to <strong>perceive complex 3D environments</strong> and take informed decisions for <strong>long-horizon tasks</strong> through Reinforcement Learning. 
                </p>
    
    At <strong></strong>IIIT-Hyderabad</strong>, I explored robotics and computer vision with <a href="https://www.iiit.ac.in/people/faculty/madhava-krishna" target="_blank">Prof. Madhava Krishna</a> and computational social science with <a href="https://www.iiit.ac.in/people/faculty/vinoo-alluri" target="_blank">Prof. Vinoo Alluri</a>, learning to approach problems from multiple angles and design interdisciplinary solutions. Currently, I'm sharpening that foundation through MRSD's unique blend of coursework, systems engineering, and entrepreneurship training which challenge me to think beyond code and about scalability, reliability, and teamwork in building complex robotic systems.
                </p>
                
                <!-- <p>
		At CMU, I'm deepening that foundation through MRSD's unique blend of AI-driven robotics coursework, systems engineering, and entrepreneurship training. My technical focus includes reinforcement learning for navigation, computer vision, and multimodal perception, while systems engineering and project management courses challenge me to think beyond code about scalability, reliability, and teamwork in building complex robotic platforms.
                </p> -->
                
                <p>
		Previously, I worked as a Pre-Doctoral Research Fellow at TCS Research where I Investigated reinforcement learning methods for embodied agents to integrate <strong></strong>audio-visual perception</strong> for spatial understanding and navigation, bypassing explicit SLAM through learned internal representations. 
                </p>
                
                <!-- <p>
    My curiosity about how the world works trumps all - I love learning and thinking about new ways to solve pressing problems in society and humanity as a whole.
                 </p> -->

                <p>
		<strong>Research Interests:</strong> 3D Computer Vision and Long-Horizon Robot Intelligence for embodied agents.
                </p>
                <p style="text-align:center">
                  <a href="mailto:sparuchu@cs.cmu.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/sreeharshaparuchuri">LinkedIn</a> &nbsp;/&nbsp;
                  <a href="https://github.com/sreeharshaparuchur1">Github</a> &nbsp;/&nbsp;
                  <!-- <a href="documents/Sreeharsha's Resume CMU Fulltime.pdf">Resume ML</a> &nbsp;/&nbsp; -->
                  <a href="documents/Sreeharsha_Resume_CMU_Fulltime_Oct25.pdf">Resume</a>

                </p>
              </div>
              <div class="profile-picture-container">
                <a href="images/profile_pictures/sreeharsha_profile_picture.JPG">
                  <img alt="profile photo" src="images/profile_pictures/sreeharsha_profile_picture.JPG">
                </a>
              </div>
            </div>
          </div>


          <!-- News Section -->
          <table id="news" style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:12px;"><tbody>
            <tr>
              <td>
                <h2>News</h2>
              </td>
            </tr>
          </tbody></table>
          <div class="news-container">
            <div class="news-item">
              <span class="news-date">Oct 2025:</span> Finished in <a href="https://www.ai-meets-autonomy.com/cmu-vla-challenge#h.tx9eukqep4hb" target="_blank">3rd place</a> in the CMU VLA Challenge and will be presenting our work at IROS 2025! üèÜ
            </div>
            <div class="news-item">
              <span class="news-date">Sep 2025:</span> Conducted an in-person lab on <a href="https://www.youtube.com/watch?v=Fb9_nDqv47A" target="_blank">Backpropagation and Training Convolutional Neural Networks</a>. üìö
            </div>
            <div class="news-item">
              <span class="news-date">Aug 2025:</span> Completed my summer internship at <a href="https://www.mach9.ai/" target="_blank">Mach9</a> in the Bay Area, focusing on 3D Computer Vision. üöÄ
            </div>
            <div class="news-item">
              <span class="news-date">Apr 2025:</span> <a href="https://www.youtube.com/watch?v=8D-E4PJrCck" target="_blank">Successfully demonstrated</a> our Apple Vision Pro + Robot assisted semi-autonomous Total Knee Arthroplasty system. ü©∫
            </div>
            <div class="news-item">
              <span class="news-date">Aug 2024:</span> Started graduate school at CMU‚Äôs Robotics Institute! ü§ñ
            </div>
            <div class="news-item">
              <span class="news-date">May 2024:</span> Submitted our paper on Audio-Visual Navigation to CoRL 2024. ü§û
            </div>
            <div class="news-item">
              <span class="news-date">Feb 2024:</span> Admitted to CMU‚Äôs MRSD program!
            </div>
            <div class="news-item">
              <span class="news-date">Feb 2024:</span> Invited to Google Research Week 2024 in Bangalore, India. üéì
            </div>
            <div class="news-item">
              <span class="news-date">Nov 2023:</span> Finished 4th place internationally in the Habitat <a href="https://aihabitat.org/challenge/2023_homerobot_ovmm/">Open Vocabulary Mobile Manipulation Challenge</a> at NeurIPS 2023.
            </div>
            <div class="news-item">
              <span class="news-date">Jul 2022:</span> Started working at TCS Research, Kolkata, on long-horizon robot navigation using Deep Reinforcement Learning.
            </div>
            <div class="news-item">
              <span class="news-date">Jul 2022:</span> Graduated from IIIT-H with honours in Robot Perception! Grateful to my family and friends for making it possible. ‚ú®
            </div>
            <div class="news-item">
              <span class="news-date">May 2022:</span> Built an autonomous sanitization robot on a budget of $5000 and <a href="https://www.iiit.ac.in/team-cerebrus-robotics-research-center-rrc-secured-second-place-at-the-artpark-robotics-challenge-conducted-by-iisc-bangalores-innovation-hub-iiith-team-cerebrus-was-led-by-suraj-bonagiri/" target="_blank">finished as runners up</a> in a nationwide competition. üßº
            </div>
          </div>          

                    <!-- Experience Section -->
          <table id="experience" style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:12px;"><tbody>
            <tr>
              <td>
                <h2>Industry Experience</h2>
              </td>
            </tr>
          </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr class="section-row">
              <td class="section-image-cell">
                <img src='images/logos/mach9-logo.jpeg' alt="Mach9 Logo" class="logo-image" onclick="window.open('https://www.mach9.ai/', '_blank')">
              </td>
              <td class="section-content-cell">
                <div class="duration-container">
                  <div class="section-title-container">
                    <strong class="section-title">Mach9</strong>
                  </div>
                  <div class="duration-tag">May 2025 - Aug 2025</div>
                </div>
                <em>Perception Software Engineering Intern</em><br>
                <strong>Focus:</strong> Developed CUDA-accelerated 2D-3D feature correspondence pipeline and fine-tuned Vision-Language Models to speed up quality assurance in outdoor surveying systems<br>
                <div class="experience-toggle">
                  <a href="#" onclick="toggleExperience('mach9-current'); return false;" id="mach9-current-toggle">View Details</a>
                </div>
                                  <div id="mach9-current-details" class="experience-details">
                    <div class="key-contributions">
                    ‚Ä¢ Developed and deployed Pavement Symbol Extraction functionality to the Digital Surveyor software via <strong>CUDA-accelarated coordinate frame transformations</strong> and segmentation masks.<br>
                    ‚Ä¢ Implemented <strong>unit and CI testing pipelines</strong> with GitHub Actions to validate CUDA kernels and vector-field clustering modules, ensuring production-grade reliability.<br>
                    ‚Ä¢ Fine-tuned a <strong>Vision-Language Model (VLM)</strong> to implement a secondary-inference pipeline to classify extracted open-set painted symbols according to user specifications.<br>
                    ‚Ä¢ Conducted 70+ controlled ablation experiments with <strong>A/B testing on Hungarian Assigner</strong> Costs, Loss weights, Model Queries, Multi-Scale Deformable Attention and encoder-decoder expressivity to boost the performance of the production model by 4%.<br>
                    ‚Ä¢ Utilised methods from Object-Detection literature to qualitatively capture a <strong>DETR-based</strong> polyline detection model's <strong>uncertainty</strong> to expedite downstream <strong>Quality Assurance</strong> and Quality Control processes, saving company and customer resources.<br>
                    <!-- <strong>Multimodal Symbol Extraction:</strong> Designed self-correcting vector-field and DBSCAN clustering algorithm to preserve instance consistency from multi-view panoptic masks. Accelerated per-point transformations by 50√ó via custom CUDA kernel<br>
                    ‚Ä¢ <strong>Vision-Language Model Inference:</strong> Designed orientation robust RAG pipeline for symbol classification using Gemini text embeddings and GPT-o3, achieving 85% F1 score on 20k samples<br>
                    ‚Ä¢ <strong>Uncertainty Estimation:</strong> Developed methods to quantify uncertainty in DETR-style vectorized 3D polyline predictions via self-calibration and Bayesian dropout -->
                    </div>
                  </div>
              </td>
            </tr>

            <tr class="section-row">
              <td class="section-image-cell">
                <img src='images/logos/tcs-logo.jpeg' alt="TCS Research Logo" class="logo-image" onclick="window.open('https://www.tcs.com/what-we-do/research', '_blank')">
              </td>
              <td class="section-content-cell">
                <div class="duration-container">
                  <div class="section-title-container">
                    <strong class="section-title">Tata Consultancy Services Research</strong>
                  </div>
                  <div class="duration-tag">July 2022 - July 2024</div>
                </div>
                <em>Pre-Doctoral Research Fellow</em><br>
                <strong>Focus:</strong> Led research in the realm of cognitive robotics, emphasizing navigation that focussed on audio-visual feature correspondence and reinforcement learning for active SLAM.<br>
                <div class="experience-toggle">
                  <a href="#" onclick="toggleExperience('tcs'); return false;" id="tcs-toggle">View Details</a>
                </div>
                <div id="tcs-details" class="experience-details">
                  <div class="key-contributions">
                  ‚Ä¢ <strong>Audio-Visual Navigation:</strong> Led development of embodied AI agent with multimodal sensing, training online RL policy with novel class-agnostic reward, reducing path length by 21%<br>
                  ‚Ä¢ <strong>Offline RL for Indoor Robot Navigation:</strong> Built simulation pipeline to collect large-scale trajectory datasets, training a Causal Decision Transformer with early multimodal fusion; integrated environment randomization, behavior cloning baselines, and replay buffer curation to improve policy robustness and sample efficiency <br>
                  ‚Ä¢ <strong>CLIP-Enhanced Scene Graphs:</strong> Designed contrastive-learning framework to compute visual-language embeddings, leveraging GNNs to model object-region relationships<br>
                  ‚Ä¢ <strong>Open Vocabulary Manipulation (NeurIPS 23):</strong> Developed active SLAM exploration algorithm conditioned on probabilistic semantic map, improving task success by 60%. <br>
                  ‚Ä¢ Volunteered for the Project Synergy initiative by TCS wherein volunteers taught written and spoken English to students in a Bangla-medium government school.
                  </div>
                </div>
              </td>
            </tr>

            <tr class="section-row">
              <td class="section-image-cell">
                <img src='images/logos/rrc_logo.png' alt="RRC IIIT-H Logo" class="logo-image" onclick="window.open('https://robotics.iiit.ac.in/', '_blank')">
              </td>
              <td class="section-content-cell">
                <div class="duration-container">
                  <div class="section-title-container">
                    <strong class="section-title">Robotics Research Center (RRC, IIIT-H)</strong>
                  </div>
                  <div class="duration-tag">Jan 2020 - June 2022</div>
                </div>
                <em>Research Assistant</em><br>
                <strong>Focus:</strong> Worked on dense 3D reconstruction and utilizing SLAM techniques such as pose-graph optimization indoor and outdoor autonomy on self-driving vehicles.<br>
                <div class="experience-toggle">
                  <a href="#" onclick="toggleExperience('rrc'); return false;" id="rrc-toggle">View Details</a>
                </div>
                <div id="rrc-details" class="experience-details">
                  <div class="key-contributions">
                  ‚Ä¢ <strong>Autonomous Sanitization Robot:</strong> Designed and implemented end-to-end robotic system during COVID-19 to autonomously sanitize indoor spaces, integrating computer vision, Visual-SLAM, and coverage-based navigation<br>
                  ‚Ä¢ <strong>Sim-to-Real Deployment:</strong> Built Gazebo simulation environments for iterative testing, then transferred stack to hardware platform with onboard sensors and sanitization actuators; finished <em>runner-up among 140 teams</em><br>
                  ‚Ä¢ <strong>LiDAR SLAM:</strong> Evaluated LiDAR odometry and mapping approaches such as LOAM using CARLA simulation and outdoor driving data, analyzing localization accuracy and map consistency<br>
                  ‚Ä¢ <strong>Depth Estimation:</strong> Implemented stereo and monocular depth estimation methods on driving datasets including KITTI and NuScenes, developing a ROS package for multi-view bundle adjustment<br>                  
                  </div>
                </div>
              </td>
            </tr>
          </tbody></table>

          <!-- Additional Experience Subsection -->
          <div class="additional-experience-section">
            <h3 style="text-align: center; font-size: 18px; font-weight: 400; color: var(--text-color); margin: 20px 0 15px 0;">I have also been a part of</h3>
            <div class="additional-logos-container">
              <div class="additional-item">
                <img src="images/logos/bosch_logo.png" alt="Bosch Research" class="additional-logo" onclick="toggleAdditional('bosch')" id="bosch-logo">
                <div id="bosch-details" class="additional-details">
                  <div class="duration-container">
                    <div class="section-title-container">
                      <strong class="section-title">Bosch Research and Technology Center</strong>
                    </div>
                    <div class="duration-tag">May 2021 - Aug 2021</div>
                  </div>
                  <em>Computer Vision Intern</em><br>
                  <!-- <strong>Focus:</strong> Developed sensor fusion algorithms for autonomous driving, improving Multi-Object Tracking with Kalman filtering and synthetic data augmentation<br> -->
                  <div class="key-contributions">
                  ‚Ä¢ Fused Laser, Camera, and Odometry data using Kalman filtering to boost online Multi-Object Tracking performance by 11% IoU on outdoor autonomous driving datasets<br>
                  ‚Ä¢ Augmented difficult-to-obtain real-world LiDAR datasets using synthetic data from generative models and physics engines, improving 3D object detection networks for outdoor scenarios<br>
                  </div>
                </div>
              </div>
              
              <div class="additional-item">
                <img src="images/logos/precog_logo-800.webp" alt="PReCoG Lab" class="additional-logo" onclick="toggleAdditional('precog')" id="precog-logo">
                <div id="precog-details" class="additional-details">
                  <div class="duration-container">
                    <div class="section-title-container">
                      <strong class="section-title">PreCog Lab, IIIT-H</strong>
                    </div>
                    <div class="duration-tag">2020 - 2021</div>
                  </div>
                  <em>Research Assistant - Information Retrieval and Computational Social Science</em><br>
                  <!-- <strong>Focus:</strong> Applied machine learning to music information retrieval and social media analysis for mental health research, resulting in publications at top-tier venues<br> -->
                  <div class="key-contributions">
                  ‚Ä¢ Applied statistical machine learning with Music Information Retrieval to analyze lyrical regularities as early indicators of mental illness; Published results at INTERSPEECH 2021<br>
                  ‚Ä¢ Scraped Reddit data to link music-sharing trends with mental health during COVID-19 using BERT embeddings and DBSCAN clustering; Published in medical journal<br>
                  </div>
                </div>
              </div>
            </div>
          </div>

          <!-- Projects/Research Section -->
          <table id="projects" style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:12px;"><tbody>
            <tr>
              <td>
                <h2>Projects & Research</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr style="background-color: var(--highlight-bg);">
      <td style="padding:16px;width:40%;vertical-align:middle">
        <img src="images/projects/capstone_1.png" alt="AR Knee Surgery Project" class="project-image">
      </td>
              <td style="padding:8px;width:60%;vertical-align:middle">
          <div class="duration-container">
            <div class="section-title-container">
              <strong class="section-title">Augmented-Reality and Robot Assisted Knee Surgery</strong>
            </div>
            <!-- <div class="duration-tag">MRSD Capstone</div> -->
          </div>
          <a href="https://mrsdprojects.ri.cmu.edu/2025teamd/">website</a> / <a href="https://github.com/KNEEpoleon">code</a><br>
          Gathered and analyzed requirements from user studies, market competition, and sponsors to inform system development. Processed 3D and RGB information from the Apple Vision Pro to detect bone models in the environment via ICP registration.<br>
          <div class="experience-toggle" style="margin-top: 8px;">
            <a href="#" onclick="toggleExperience('ar-knee'); return false;" id="ar-knee-toggle">View Details</a>
          </div>
          <div id="ar-knee-details" class="experience-details">
            <div class="key-contributions">
              ‚Ä¢ <strong>Project Leadership:</strong> Led a 5-person team as Project Manager, driving scheduling, sponsor communication, and system integration for an AR-assisted surgical robotics platform<br>
              ‚Ä¢ <strong>Accuracy-driven Perception:</strong> Achieved sub-4 mm drilling accuracy in total knee arthroplasty using a KUKA MED7 arm with multi-stage pointcloud registration (SAM2 + ICP)<br>
              ‚Ä¢ <strong>AR Integration:</strong> Integrated Apple Vision Pro for dynamic bone tracking and real-time surgeon-in-the-loop planning across long surgical horizons<br>
              ‚Ä¢ <strong>Motion Planning:</strong> Designed and deployed a ROS + MoveIt planning subsystem that adaptively updates as surgical pins are drilled, enabling safe trajectory generation<br>
              ‚Ä¢ <strong>Hardware Development:</strong> Built a custom 3D-printed drill end-effector with embedded control electronics, activated via trajectory execution for autonomous drilling<br>
            </div>
          </div>
        </td>
    </tr>

    <tr>
      <td style="padding:16px;width:40%;vertical-align:middle">
        <img src='images/projects/ReMOV3R.png' alt="3D Foundation Models Project" class="project-image">
      </td>
      <td style="padding:8px;width:60%;vertical-align:middle">
        <div class="duration-container">
          <div class="section-title-container">
            <strong class="section-title">3D Foundation-Models for Monocular Video Reconstruction</strong>
          </div>
          <!-- <div class="duration-tag">CMU</div> -->
        </div>
        <a href="https://drive.google.com/file/d/1GlR9XqQvIqDgGwtfmFA7U9m0eOH5hUrs/view?usp=sharing">report</a> / <a href="https://github.com/sreeharshaparuchur1/Mov3r-L3D-project">code</a><br>
        Implemented semantic-geometric feature fusion using cross-attention between foundation model embeddings (DINOv2, Depth Anything) in a hierarchical state representation to recover camera extrinsics. Devised an adaptive keyframe selection strategy for confidence-aware pointmap refinement using a DUST3R-style architecture.<br>
        <div class="experience-toggle" style="margin-top: 8px;">
          <a href="#" onclick="toggleExperience('3d-foundation'); return false;" id="3d-foundation-toggle">View Details</a>
        </div>
        <div id="3d-foundation-details" class="experience-details">
          <div class="key-contributions">
            ‚Ä¢ <strong>Foundation Model Fusion:</strong> Designed cross-attention mechanism to combine DINOv2 semantic features with Depth Anything geometric priors, achieving robust 3D scene understanding from monocular video<br>
            ‚Ä¢ <strong>Adaptive Keyframe Selection:</strong> Developed confidence-aware algorithm that dynamically selects optimal frames for reconstruction, improving pointmap quality by 30% over uniform sampling<br>
            ‚Ä¢ <strong>DUST3R Architecture:</strong> Implemented hierarchical state representation with multi-scale feature pyramids to handle camera motion estimation and dense 3D reconstruction simultaneously
          </div>
        </div>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:40%;vertical-align:middle">
        <img src='images/projects/cmu_vln_1.png' alt="CMU VLA Challenge" class="project-image">
      </td>
      <td style="padding:8px;width:60%;vertical-align:middle">
        <div class="duration-container">
          <div class="section-title-container">
            <strong class="section-title">CMU VLA Challenge</strong>
          </div>
        </div>
        <a href="https://www.ai-meets-autonomy.com/cmu-vla-challenge">problem</a> / <a href="https://github.com/gupta-ish/CMU-VLA-Challenge/tree/team-copypasta-submission">code</a><br>
        Built a Vision-Language Navigation (VLN) system that answered natural language queries by combining <code>Gemini 2.5 Pro</code> embodied reasoning with a custom ROS state machine. The system produced numerical answers, object references, or waypoint plans under a strict 10-minute limit.
        <div class="experience-toggle" style="margin-top: 8px;">
          <a href="#" onclick="toggleExperience('vla-challenge'); return false;" id="vla-challenge-toggle">View Details</a>
        </div>
        <div id="vla-challenge-details" class="experience-details">
          <div class="key-contributions">
            ‚Ä¢ <strong>Natural Language Understanding:</strong> ‚Äúclosest to the window‚Äù used <code>Gemini 2.5 Pro</code> to classify and intelligently reason over spatial relations<br>
            ‚Ä¢ <strong>State Machine</strong>: designed a ROS state machine to coordinate exploration, mapping, and answering with dynamic transitions<br>
            ‚Ä¢ <strong>Deployment Ready</strong>: Deployed the system on a real robot for the challenge through clean docker containerization.
          </div>
        </div>
      </td>
    </tr>


    <tr>
      <td style="padding:16px;width:40%;vertical-align:middle">
        <img src='images/projects/poseGraph.png' alt="Pose Graph Optimization Project" class="project-image">
      </td>
      <td style="padding:8px;width:60%;vertical-align:middle">
        <div class="duration-container">
          <div class="section-title-container">
            <strong class="section-title">Pose Graph Optimization for 2D SLAM</strong>
          </div>
        </div>
        <a href="link-to-report">report</a> / <a href="link-to-code">code</a><br>
        Implemented a 2D SLAM backend where noisy odometry and loop closure constraints were refined into a globally consistent trajectory. Used <code>jax</code> to compute residuals and Jacobians, applied nonlinear least-squares optimization, and validated improvements with RPY and APE error metrics. Explored the role of confidence weighting in the information matrix and compared against <code>g2o</code> optimization with robust kernels.
        <div class="experience-toggle" style="margin-top: 8px;">
          <a href="#" onclick="toggleExperience('pose-graph'); return false;" id="pose-graph-toggle">View Details</a>
        </div>
        <div id="pose-graph-details" class="experience-details">
          <div class="key-contributions">
            ‚Ä¢ <strong>Iterative Optimization:</strong> Built custom nonlinear solver in JAX with residual and Jacobian computation for pose updates<br>
            ‚Ä¢ <strong>Loop Closures:</strong> Studied effect of varying odometry vs loop closure confidence weights on trajectory quality visualized in <strong>g2o_viewer </strong><br>
            ‚Ä¢ <strong>Error Evaluation:</strong> Quantified improvements via RPY drift and Absolute Pose Error reduction compared to initial odometry<br>
            ‚Ä¢ <strong>Literature Review:</strong> Analyzed ‚ÄúPast, Present & Future of SLAM‚Äù survey, contextualizing open problems in robustness and scalability with deep learning-based approaches
          </div>
        </div>
      </td>
    </tr>
    

    <tr style="background-color: var(--highlight-bg);">
      <td style="padding:16px;width:40%;vertical-align:middle">
        <img src='images/projects/reddit_2.png' class="project-image">
      </td>
      <td style="padding:8px;width:60%;vertical-align:middle">
        <div class="duration-container">
          <div class="section-title-container">
            <strong class="section-title">Music, Mental Health, and Representation Learning</strong>
          </div>
          <!-- <div class="duration-tag">IIIT-H</div> -->
        </div>
        <a href="https://pubmed.ncbi.nlm.nih.gov/37471415/">publication</a> / <a href="#">code</a><br>
        Applied BERT-based sentiment analysis and k-means clustering to uncover nuanced links between language and acoustic music features in data scraped from mental health related subreddits during COVID-19. This research contributed to understanding the relationship between music and mental health through computational methods.
        <div class="experience-toggle" style="margin-top: 8px;">
          <a href="#" onclick="toggleExperience('music-mental-health'); return false;" id="music-mental-health-toggle">View Details</a>
        </div>
        <div id="music-mental-health-details" class="experience-details">
          <div class="key-contributions">
            ‚Ä¢ <strong>BERT Sentiment Analysis:</strong> Fine-tuned transformer models on mental health discourse to extract emotional patterns from 50k+ Reddit posts, achieving 87% accuracy in mood classification<br>
            ‚Ä¢ <strong>Music Information Retrieval:</strong> Developed acoustic feature extraction pipeline using librosa and essentia to correlate musical elements (tempo, key, valence) with psychological states<br>
            ‚Ä¢ <strong>COVID-19 Impact Study:</strong> Applied k-means clustering and statistical analysis to identify significant behavioral shifts in music consumption patterns during pandemic, published findings at INTERSPEECH 2021
          </div>
        </div>
      </td>
    </tr>

          </tbody></table>

          <!-- View More Projects Link -->
          <div class="view-more-container">
            <a href="projects.html" class="view-more-link">View More Projects ‚Üí</a>
          </div>


          <!-- Education Section -->
          <table id="education" style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:12px;"><tbody>
            <tr>
              <td>
                <h2>Education</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
                          <tr class="section-row">
                <td class="section-image-cell">
                  <img src='images/logos/cmu.jpeg' alt="Carnegie Mellon University Logo" class="education-logo" onclick="window.open('https://www.ri.cmu.edu/', '_blank')">
                </td>
              <td class="section-content-cell">
                <div class="duration-container">
                  <div class="section-title-container">
                    <strong class="section-title">Carnegie Mellon University, School of Computer Science</strong>
                  </div>
                  <div class="duration-tag">Aug 2024 - May 2026</div>
                </div>
                <em>Master of Science in Robotic Systems Development (MRSD)</em><br>
                <strong>CGPA: 4.11/4.0</strong><br>
                <strong>Teaching:</strong> Introduction to Deep Learning<br>
                <strong>Coursework:</strong> <span id="cmu-courses-short">Learning for 3D Vision, Generative AI, Deep RL</span>
                <div class="experience-toggle">
                  <a href="#" onclick="toggleExperience('cmu'); return false;" id="cmu-toggle">View Details</a>
                </div>
                <div id="cmu-details" class="education-details">
                  <div class="education-contributions">
                    ‚Ä¢ <strong>Learning for 3D vision:</strong> 3D generation, volume rendering + NeRFs, gaussian splatting + diffusion-guided optimization, Classifier-Free Guidance, PointNet classification and segmentation<br>
                    ‚Ä¢ <strong>Generative AI:</strong> grouped query attention + RoPE in GPT-2, diffusion models and VAEs, parameter-efficient fine-tuning + DPO, In-Context Learning, Mixture of Experts, paper review<br>
                    ‚Ä¢ <strong>Deep reinforcement learning:</strong> policy gradients, Q-learning, Performance Difference Lemma, actor-critic methods, Proximal Policy Optimization, evolutionary methods, DAgger, Imitation Learning<br>
                    ‚Ä¢ <strong>Robot autonomy, mobility and control:</strong> grasping, Kalman filtering, control for drones, humanoids<br>
                    ‚Ä¢ <strong>Advanced computer vision:</strong> homography, Lucas-Kanade tracking, photometric stereo<br>
                    ‚Ä¢ <strong>Systems engineering:</strong> functional architecture, unit tests, project management<br>
                  </div>
                </div>
              </td>
            </tr>

            <tr class="section-row">
                              <td class="section-image-cell">
                  <img src='images/logos/iiith-v3.png' alt="IIIT Hyderabad Logo" class="education-logo" onclick="window.open('https://www.iiit.ac.in/', '_blank')">
                </td>
              <td class="section-content-cell">
                <div class="duration-container">
                  <div class="section-title-container">
                    <strong class="section-title">International Institute of Information Technology, Hyderabad</strong>
                  </div>
                  <div class="duration-tag">Aug 2018 - July 2022</div>
                </div>
                <em>Bachelor of Technology in Electronics and Communication Engineering (Honours)</em><br>
                <strong>Major GCPA: 9.02/10</strong><br>
                <strong>Awards:</strong> Deans Merit List, Undergraduate Research Award<br>
                <strong>Coursework:</strong> <span id="iiith-courses-short">Statistics in AI, Topics in Applied Optimization, Mobile Robotics</span>
                <div class="experience-toggle">
                  <a href="#" onclick="toggleExperience('iiith'); return false;" id="iiith-toggle">View Details</a>
                </div>
                <div id="iiith-details" class="education-details">
                  <div class="education-contributions">
                    ‚Ä¢ <strong>Statistical methods in AI:</strong> PCA, SVMs, Bayesian inference, logistic regression, image classification<br>
                    ‚Ä¢ <strong>Applied optimization:</strong> linear programming, convex optimization, singular value decomposition<br>
                    ‚Ä¢ <strong>Computer vision:</strong> camera calibration, SIFT, grab-cut, Mask-RCNNs, bag of words, Viola-Jones<br>
                    ‚Ä¢ <strong>Mobile robotics:</strong> pose-graph optimization, epipolar geometry, RRT, non-linear optimization<br>
                    ‚Ä¢ <strong>Digital image processing:</strong> edge detection, morphological operations, alpha blending<br>
                    ‚Ä¢ <strong>Data structures and algorithms:</strong> graph algorithms, dynamic programming, complexity analysis<br>
                    ‚Ä¢ <strong>Operating systems:</strong> process management, memory allocation, file systems, concurrency<br>
                    ‚Ä¢ <strong>Linear algebra:</strong> matrix operations, eigenvalues, vector spaces, linear transformations<br>
                    ‚Ä¢ <strong>Compilers:</strong> lexical analysis, parsing, code generation and grammar<br>
                    ‚Ä¢ <strong>Game theory:</strong> Nash equilibrium, mechanism design<br>
                  </div>
                </div>
              </td>
            </tr>
          </tbody></table>


          <!-- Teaching Section -->
          <table id="teaching" style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:12px;"><tbody>
            <tr>
              <td>
                <h2>Teaching Experience</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
            <tr class="section-row">
              <td class="section-image-cell">
                <img src='images/logos/cmu.jpeg' alt="Carnegie Mellon University Logo" class="education-logo" onclick="window.open('https://deeplearning.cs.cmu.edu/', '_blank')">
              </td>
              <td class="section-content-cell">
                <div class="duration-container">
                  <div class="section-title-container">
                    <strong class="section-title">Carnegie Mellon University</strong>
                  </div>
                  <!-- <div class="duration-tag">Fall 2025</div> -->
                </div>
                <em>Introduction to Deep Learning (11-785)</em><br>
                <a href="https://deeplearning.cs.cmu.edu/" target="_blank">Course Website</a><br>
                <strong>Description:</strong> Comprehensive graduate-level course covering neural networks, CNNs, RNNs, transformers, and modern deep learning architectures.
                <div class="experience-toggle">
                  <a href="#" onclick="toggleExperience('cmu-teaching'); return false;" id="cmu-teaching-toggle">View Details</a>
                </div>
                <div id="cmu-teaching-details" class="experience-details">
                  <div class="key-contributions">
                    ‚Ä¢ Created educational content including slides and tutorials for <a href="https://www.youtube.com/watch?v=pAVnXfgRJIA" target="_blank">NumPy fundamentals</a> and <a href="https://www.youtube.com/watch?v=yTQSvcuRHro" target="_blank">Loss functions (Focal Loss, Chamfer Loss, RLHF)</a><br>
                    ‚Ä¢ Designed a Colab Notebook and made slides to lead a lab on <a href="https://www.youtube.com/watch?v=Fb9_nDqv47A" target="_blank">Backprop and Training Convolutional Neural Networks</a>.<br>
                    ‚Ä¢ Collaborated with instructional team to revise and update homework assignments for RNNs, GRUs, Transformers, Language Generation, Diffusion models and PEFT<br>
                    ‚Ä¢ Conducted over 40 hours of office hours, labs, and hackathon events, providing hands-on instruction and problem-solving support for undergraduate and graduate students
                  </div>
                </div>
              </td>
            </tr>

            <tr class="section-row">
              <td class="section-image-cell">
                <img src='images/logos/iiith-v3.png' alt="IIIT Hyderabad Logo" class="education-logo" onclick="window.open('https://www.iiit.ac.in/', '_blank')">
              </td>
              <td class="section-content-cell">
                <div class="duration-container">
                  <div class="section-title-container">
                    <strong class="section-title">International Institute of Information Technology, Hyderabad</strong>
                  </div>
                  <!-- <div class="duration-tag">2020 - 2022</div> -->
                </div>
                <em>Multiple Courses</em><br>
                <strong>Courses:</strong> Mobile Robotics, Music Mind and Technology, Introduction to Coding Theory<br>
                <div class="experience-toggle">
                  <a href="#" onclick="toggleExperience('iiith-teaching'); return false;" id="iiith-teaching-toggle">View Details</a>
                </div>
                <div id="iiith-teaching-details" class="experience-details">
                  <div class="key-contributions">
                    ‚Ä¢ <strong>CS7.503.M21: Mobile Robotics</strong>: The most renowned course of IIIT-H. Provides students with a comprehensive toolkit for research at the intersection of Robotics and Computer Vision, covering SLAM, Computer Vision and Planning algorithms<br>
                    ‚Ä¢ <strong>CS9.434.S22: Music, Mind and Technology</strong>: An interdisciplinary course using algorithms and mathematics to explore how music is perceived by individuals and groups. Served as head TA, designing evaluations for over 60 graduate and undergraduate students<br>
                    ‚Ä¢ <strong>EC5.205.S21: Introduction to Coding Theory</strong>: A fascinating subject building on Shannon's Theory of Communication, exploring the mathematical foundations that underpin everyday communication systems<br>
                    <!-- ‚Ä¢ <strong>RRC Summer School</strong>: Taught students the fundamentals of classical Computer Vision and introduced modern AI-based methods utilizing convolutional neural networks during this intensive summer program -->
                  </div>
                </div>
              </td>
            </tr>
          </tbody></table>


          <!-- Skills Section -->
          <table id="skills" style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;"><tbody>
            <tr>
              <td>
                <h2>Technical Skills</h2>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(300px, 1fr)); gap: 20px;">
                  <div>
                    <strong>Languages:</strong> Python, C++, MATLAB, CUDA, Java, Go, Swift<br>
                    <strong>ML/AI:</strong> PyTorch, TensorFlow, Scikit-learn, PyTorch3D
                  </div>
                  <div>
                    <strong>Tools:</strong> ROS2, Unity 3D, OpenCV, XCode, Django, Git<br>
                    <strong>Miscellaneous:</strong> Rust, JAX, Docker, Kubernetes, AWS, GCP
                  </div>
                </div>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Original Template taken from <a href="https://github.com/jonbarron/jonbarron_website">here!</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html> 